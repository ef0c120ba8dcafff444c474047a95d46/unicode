---
title: web
layout: default
---
 
## Unicode. UTF-8, UTF-16, UTF-32

### Содержание

  * **[История возникновения](#история-возникновения)**
  * **[Unicode сегодня](#unicode-сегодня)**
  * **[Unicode Transformation Format](#unicode-transformation-format)**
  * **[UTF-8](#utf-8)**
  * **[UTF-16](#utf-16)**
  * **[UTF-32](#utf-32)**<br>
  * **[UTF-16 LE, UTF-16 BE, UTF-32 LE,  UTF-32 BE](#utf-16-le-utf-16-be-utf-32-le--utf-32-be)**
  * **[Комбинируемые символы и Нормализация](#комбинируемые-символы-и-нормализация)**
  * **[Заключение](#заключение)**
  * **[Список использованных источников](#список-использованных-источников)**


### История возникновения

 К 70-м годам XX века компьютеры из узкоспециализированной ниши "вычислителей", которые использовались для математических расчетов, (а в английском одно из значений слова computer — вычислительная машина), начинают широко использоваться обычными пользователями в обработке текстовой информации. Но буквы и другие символы это визуальный объект, все с чем имели компьютеры дело до этого — цифры. 
 
 При работе с символами возникли новые задачи и основная из них — преобразование. При обработке символов компьютеры представляют их числами, а при выводе информации используются таблицы кодировки символов (англ. encoding character set) для преобразования символов в числа и обратно. Эта задача оказалась не тривиальной, со временем появились сотни таблиц символов со своими особенностями и проблемами совместимости. 
 
 Например, в СССР и затем в России были разработаны кодировки КОИ-7, КОИ-8 (Коды Обмена Информацией), использовавшие соответственно 7 или 8 бит для представления одного символа, а также CP866 (основанная на CP437 от IBM). Таблица кодировки ASCII (англ. American standard code for information interchange) была разработана в США в 1963 году. Изначально в ASCII включены 128 символов — она использует все комбинации семи битов (от 0000000 до 1111111), а 8 бит был контрольным для проверки ошибок возникающих при передаче данных.
  
 Лингвисты, ученые, инженеры,  пользователи, которые занимались обработкой многоязычных текстов, формул,  сталкивались с проблемами при редактировании и передаче информации. Текст подготовленный с использованием одного программного обеспечения (ПО)  было затруднительно корректно обработать на другом. Таблица кодировки, использованная для подготовки документа, могла не содержать необходимых  символов. Более того, зачастую таблицы кодировки были не совместимы друг с другом, например, использовали один код для представления разных символов.
 
 В 1991 году некоммерческая организация "Консорциум Юникода" (англ. Unicode Consortium) предложила стандарт кодирования символов, позволяющий представить символы практически всех письменных языков мира — Unicode. 
  
  Идея была в том, чтобы создать универсальный набор символов (англ. universal character set UCS) и однозначно ему сопоставить семейство кодировок (англ. Unicode transformation set UTF). Семейство кодировок используется для машинного представления символов.
  
 В наши дни в некоммерческую организацию "Консорциум Юникода" входят Adobe, Apple, Facebook, Google, IBM, Microsoft, SAP, а Unicode  используется в XML, Java, JavaScript, LDAP, CORBA 3.0, WML. Именно Unicode является официальной схемой реализации стандарта ISO 10646.
 
### Unicode сегодня

  20 Марта 2020 года, была анонсирована версия Unicode 13.0.0 Были добавлены 5 930 новых символов, таким образом общее количество символов в актуальной версии Unicode 143 859 и 3 304 Эмодзи.  Unicode позволяет закодировать символы современных и ныне не используемых систем письменности, включая иероглифы, математические символы и символы нотной нотации. И все это в одном семействе кодировок.

### Unicode Transformation Format 

 Универсальный набор символов (UCS) ставит в соответствие каждому символу число в шестнадцатеричной системе счисления с префиксом U+
 
```
Символ	Bin	 		Hex
A	0000000001000001	0x0041

```

Таким образом заглавный латинский символ A кодируется как U+0041. Первые 128 символов в наборе символов (UCS) соответствуют ASCII. 

 Существуют несколько основных кодировок Unicode (UTF)  UTF-8, UTF-16 (UTF-16 LE и UTF-16 BE), UTF-32(UTF-32 LE и UTF-32 BE). Каждая кодировка использует разное количество байт для представления символов и имеет свои особенности. 
 
  
#### UTF-8 

UTF-8 отличается компактностью благодаря использованию плавающего количества байт (от 1 до 4) для кодировки символов. UTF-8 обеспечивает наилучшую совместимость с унаследованными 8 битными кодировками. UTF-8 организована таким образом, чтобы часто используемые символы кодировались 1 или 2 байтами, благодаря этому тексты в этой кодировке компактны.  Количество байт для кодировки символа определено в RFC 3629:

```
Номер символа в UCS	Количество байт для кодировки
00000000—0000007F	1
00000080—000007FF	2
00000800—0000FFFF	3
00010000—0010FFFF	4
```
 
 Количество байт для кодировки определяет первые биты:
  * 0xxxxxxx — если для кодирования потребуется один байт
  * 110xxxxx — если для кодирования потребуется два байта
  * 1110xxxx — если для кодирования потребуется три байта
  * 11110xxx — если для кодирования потребуется четыре байта

Если для кодирования требуется более одного байта, то старшие биты каждого последующего байта устанавливаются в 10 (10xxxxxx), чтобы отличать следующий байт в последовательности.

Для примера рассмотрим кодирование символа **Ю** в UTF-8. В USC ему соответствует код:

```
hex 0x042E  
bin 10000 101110 
```
 
Unicode в двоичном виде разбивается на две части: пять левых бит и шесть правых. К первой части добавляется признак 110, так как для кодирования потребуется 2 байта, потому что символ Ю входит в  диапазон 00000080—000007FF UCS.

```
bin ***110***10000 101110 
```

К второй части, добавляется признак 10 (первые 2 бита второго байта), которые указывают на продолжение последовательности из 2 байт.

```
bin ***110***10000 ***10***101110 
```

Таким образом, символ **Ю** кодируется как 

```
bin 11010000 10101110 
```

или

```
hex 0xD0AE 
```

и отображается в UTF-8 как **U+D0AE**

#### UTF-16 
 
 UTF-16 для кодирования символа использует от 2 до 4 байт. Наименьшей единицей представления символа является 2 байта — такую последовательность называют "словом". Символы UCS от U+0000 до U+FFFF занимают 2 байта, символы от U+10000 до U+10FFFF занимают 4 байта.

 В UTF-16 выделен диапазон от U+D800 до U+DFFF, он используется для кодирования символов, которые представляются двумя "словами" (4 байтами), такие пары называют суррогатными. Если при преобразовании пары "слов" в шестнадцатеричный код получается число из этого диапазона, это суррогатная пара.
 
Символы UCS от U+0000 до U+FFFF (исключая диапазон для суррогатов) записываются 16 битным "словом".

Символы в диапазоне U+10000 до U+10FFFF (2 "слова") кодируются по следующей схеме:
  * из кода символа вычитается 0x10000. В результате получится значение от нуля до FFFFF, которое занимает максимум 20 бит
  * результат разделите на 0x400 и прибавьте 0xD800, целая часть от значения будет первым "словом", которое входит в диапазон от 0xD800 до 0xDBFF
  * младшие 10 бит суммируются с 0xDC00, результат будет вторым "словом", которое входит в диапазон от 0xDC00 до 0xDFFF

Для примера рассмотрим кодирование символа **𐍂** (Gothic Letter Raida) в UTF-16. В USC ему соответствует код:

```
hex 0x10342
bin 0001 0000 0011 0100 0010
```

Вычтем из кода символа 0x10000

```
0x10342 - 0x10000 = 0x0342
```

Результат разделим на 0x400

```
0x0342 / 0x400 = 0x0,d08
```

и возьмем от него целую часть
 
```
0x0
```


Для получения первого суррогата, прибавим 0xd800

```
0x0 + 0xd800 = 0xd800
```

Для получения второго суррогата, возьмем младшие 10 бит

```
1101000010 = 0x0342
```

Прибавим 0xdc00

```
0x0342 + 0xdc00 = 0xdf42
```

Таким образом символ **𐍂** U+10342 в UTF-16 кодируется как

```
0xd800  0xdf42 
```

В обоих словах старшие 6 бит используются для обозначения суррогата. 

* **110110**0000000000 **110111**1101000010

А 10-й бит содержит 0 у лидирующего слова и 1 у второго. В связи с этим можно легко определить последовательность суррогатов.

* 110110000**0**000000 110111110**1**000010



#### UTF-32

В отличии от UTF-8 и UTF-16, которые используют переменное количество байт для кодирования символов, UTF-32 всегда использует 4 байта при кодировании.
 
Главным преимуществом UTF-32 является простота операций с символами, из-за предсказуемой длины кода символа они легко индексируются, а операция по замене символов в подстроке становится простой, так как можно использовать целое число в качестве индекса.

Однако за такое удобство UTF-32 расплачивается размером файла и оперативной памяти необходимой для обработки текста.

В сравнении с UTF-16, где наиболее часто используемые символы кодируются 2 байтами, UTF-32 неэффективно использует ресурсы.
 
#### UTF-16 LE, UTF-16 BE, UTF-32 LE,  UTF-32 BE

Один символ кодировки UTF-16 UTF-32 представлен последовательностью двух байт (одно "слово") или двух пар байт (4 байта, 2 "слова"). Какой из двух байт в слове первый, старший или младший, зависит от порядка. Суффиксы BE и LE в названии кодировки как раз указывают на порядок. 

  * BE — big-endian (от старшего к младшему)
  * LE — little-endian (от младшего к старшему)
  
Byte Order Mark (BOM) специальный символ U+FEFF, используется для того, чтобы определить порядок байт (BE, LE). Согласно спецификации этот символ значимый только в начале файла или потока. Основываясь на  BOM можно определить кодировку в последовательности символов Unicode.

```
Кодировка	BOM
UTF-8		EF BB BF
UTF-16 (BE)	FE FF
UTF-16 (LE)	FF FE
UTF-32 (BE)	00 00 FE FF
UTF-32 (LE)	FF FE 00 00
```

#### Комбинируемые символы и Нормализация

Символы в UCS могут быть не только базовыми (кодируемыми одним символом), но и комбинируемыми. Классический пример символ й, может быть представлен в базовом виде и комбинируемом:

  * **й**  U+0419 в виде базового символа 
  * **и** +  ̆  в виде комбинируемого символа U+0418 U+0306
  
Подобный "дуализм" предоставляет потенциальную возможность совершить ошибку при обработке данных. Для решения этой проблемы ввели процедуру нормализации Unicode.

Алгоритмы нормализации  (англ. normalization forms), созданы для приведения символов Unicode к определенному стандартному виду, в интересах последующей идемпотентной обработки. Это достигается путем замены символов на эквивалентные в соответствии с подготовленными заранее правилами.

Алгоритмы нормализации включают процесс декомпозиции — разложения символа, и композиции — соединение нескольких символов в один.

Различают четыре стандартных алгоритма нормализации:

  * **NFD** (Normalization Form Canonical Decomposition).  
    * Предполагает рекурсивное разложение составных символов в соответствии с таблицами декомпозиции, пока все символы в последовательности не станут базовыми.
  * **NFC** (Normalization Form Canonical Composition). 
    * Предполагает последовательное выполнение декомпозиции по алгоритму NFD и композиции.
  * **NFKD** (Normalization Form Compatibility Decomposition).
    * Предполагает последовательное выполнение декомпозиции и замены на почти эквивалентные символы в соответствии с таблицами совместимой декомпозиции.
  * **NFKC** (Normalization Form Compatibility Composition).
    * Предполагает последовательное выполнение декомпозиция по алгоритму NFKD и NFC.


Использование нормализации является обязательным для снижения риска подвернуться атаке злоумышленников использующих особенности комбинируемых символов Unicode.

### Заключение

Проведем эксперимент, чтобы подкрепить выводы: сохраним заглавную страницу русской и английской версии Википедии в кодировках UTF-8, UTF-16, UTF-32. Заглавная страница Википедии (на английском языке):

  * в кодировке UTF-8  80  килобайт
  * в кодировке UTF-16 160 килобайт
  * в кодировке UTF-32 319 килобайт
  

Заглавная страница Википедии (русская версия):

  * в кодировке UTF-8  129 килобайт
  * в кодировке UTF-16 230 килобайт
  * в кодировке UTF-32 460 килобайт


Любая Unicode кодировка позволяет закодировать все символы UCS и конвертация между различными кодировками осуществляется без потерь информации. При преимущественной обработке текста с латиницей, разумно выбрать кодировку UTF-8, это позволит оптимально использовать ресурсы, так как для кодирования символов будет использоваться 1-2 байта.
UTF-16 покажет оптимальные результаты при работе с мультиязычными текстами. UTF-32 неэффективно использует ресурсы, но возможно это будет компенсировано простотой и скоростью обработки.

В целом UTF-16 более подходит для внутреннего представления данных в программном обеспечении, так как в этом случае проблема порядка (LE, BE) отсутствует - принимается соглашение, какой будет использоваться, при этом индексация выполняется быстрее, единственный аспект, который необходимо учитывать - это правильная обработка суррогатных пар.

### Список использованных источников

  * [http://www.Unicode.org/versions/Unicode13.0.0/](http://www.Unicode.org/versions/Unicode13.0.0/)
  * [https://www.Unicode.org/emoji/charts-13.0/emoji-counts.html](https://www.Unicode.org/emoji/charts-13.0/emoji-counts.html)
  * [https://www.Unicode.org/standard/principles.html](https://www.Unicode.org/standard/principles.html)
  * [http://www.Unicode.org/standard/translations/russian.html](http://www.Unicode.org/standard/translations/russian.html)
  * [https://ru.bmstu.wiki/Unicode ](https://ru.bmstu.wiki/Unicode)
  * [https://dic.academic.ru/dic.nsf/ruwiki/8184](https://dic.academic.ru/dic.nsf/ruwiki/8184)
  * [https://www.iso.org/ru/standard/69119.html](https://www.iso.org/ru/standard/69119.html)
  * [https://medium.com/@kozlova14/%D0%BE%D0%B1%D1%89%D0%B5%D0%B5-%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BE-unicode-utf-8-utf-16-le-be-bom-65be4e13b57e](https://medium.com/@kozlova14/%D0%BE%D0%B1%D1%89%D0%B5%D0%B5-%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BE-unicode-utf-8-utf-16-le-be-bom-65be4e13b57e)
  * [https://i.voenmeh.ru/kafi5/Kam.loc/inform/UTF-8.htm](https://i.voenmeh.ru/kafi5/Kam.loc/inform/UTF-8.htm)
  * [https://github.com/DmitryTsybulkin/unicode/blob/master/README.md](https://github.com/DmitryTsybulkin/unicode/blob/master/README.md)
  * [https://www.compart.com/en/unicode/U+10342](https://www.compart.com/en/unicode/U+10342)


